import os
import threading
import asyncio
import logging
from typing import Optional, List, Dict
from datetime import date
from .storage import LakeStorage
from .downloader import LakeTaskScheduler

logger = logging.getLogger(__name__)

class LakeManager:
    """
    Data Lake V2 ç®¡ç†ä¸­å¿ƒ
    èšåˆå­˜å‚¨ã€ä¸‹è½½ã€è°ƒåº¦ç­‰åŠŸèƒ½ã€‚
    """
    _instance: Optional['LakeManager'] = None
    _lock = threading.Lock()

    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(LakeManager, cls).__new__(cls)
                cls._instance._initialized = False
            return cls._instance

    def __init__(self):
        if self._initialized:
            return
            
        self.storage = LakeStorage()
        self.scheduler = LakeTaskScheduler(self.storage, max_workers=15)
        self._last_summary = None
        self._last_summary_time = 0
        self._initialized = True

    async def get_top_pairs(self, limit: int = 100, rank_type: str = "market_cap") -> List[str]:
        """è·å–å¸‚åœºæ’åäº¤æ˜“å¯¹ï¼Œæ”¯æŒå¸‚å€¼ (market_cap) å’Œæˆäº¤é¢ (volume)"""
        from .fetcher import BinanceFetcher
        # è¿™é‡Œé»˜è®¤å°è¯•ä½¿ç”¨å®¿ä¸»æœºä»£ç†ä»¥é˜² API è¿æ¥å¤±è´¥
        fetcher = BinanceFetcher(proxy_config="http://host.docker.internal:7890")
        if rank_type == "market_cap":
            return await fetcher.get_market_cap_pairs(limit)
        else:
            return await fetcher.get_top_trading_pairs(limit)

    def start_download(self, pairs: List[str], intervals: List[str], start_date: date, end_date: date):
        """
        çµæ´»ä¸‹è½½å…¥å£ï¼šæ”¯æŒå¤šå¸ç§ã€å¤šå‘¨æœŸã€ä»»æ„æ—¥æœŸã€‚
        """
        def _bg_run():
            self.scheduler.add_tasks(pairs, intervals, start_date, end_date)
            self._run_async_tasks()
            
        threading.Thread(target=_bg_run, daemon=True).start()

    def stop_download(self):
        """å¼ºåˆ¶ç»ˆæ­¢ä¸‹è½½ä»»åŠ¡"""
        self.scheduler.cancel_tasks()

    def pause_download(self):
        """æš‚åœä¸‹è½½"""
        self.scheduler.pause_tasks()

    def resume_download(self):
        """æ¢å¤ä¸‹è½½"""
        self.scheduler.resume_tasks()

    def is_paused(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¤„äºæš‚åœçŠ¶æ€"""
        return not self.scheduler._pause_event.is_set()

    def auto_fill_history(self, pairs: List[str], intervals: List[str], years: int = 3):
        """
        ä¸€é”®è¡¥é½å†å²æ•°æ®é€»è¾‘ã€‚
        """
        def _bg_run():
            # auto_fill_gaps ç°åœ¨æ˜¯åŒæ­¥çš„æˆ–ç®€å•çš„åŒ…è£…
            asyncio.run(self.scheduler.auto_fill_gaps(pairs, intervals, years))
            self._run_async_tasks()
            
        threading.Thread(target=_bg_run, daemon=True).start()

    def retry_failed_tasks(self):
        """é‡è¯•æ‰€æœ‰å¤±è´¥çš„ä»»åŠ¡"""
        failed_tasks = [t for t in self.scheduler.tasks if t.status == "failed"]
        if not failed_tasks:
            logger.info("No failed tasks to retry.")
            return
            
        logger.info(f"Retrying {len(failed_tasks)} failed tasks...")
        for t in failed_tasks:
            t.status = "pending"
            t.error = None
            t.rows_downloaded = 0
            
        # ç¡®ä¿è°ƒåº¦å™¨å¯ä»¥è¿è¡Œ (å¦‚æœæ˜¯åœæ­¢çŠ¶æ€)
        self.scheduler._stop_signal = False
        self.scheduler._pause_event.set()
        
        # å¯åŠ¨åå°çº¿ç¨‹ (å¦‚æœå·²æœ‰çº¿ç¨‹åœ¨è·‘ _run_scheduler ä¼šè‡ªåŠ¨æ¡èµ· pending ä»»åŠ¡å—ï¼Ÿ
        # å–å†³äº scheduler.run å®ç°ã€‚scheduler.run æ˜¯ä¸€æ¬¡æ€§çš„ gather(pending)ï¼Œè·‘å®Œå°±é€€å‡ºäº†ã€‚
        # æ‰€ä»¥å¿…é¡»é‡æ–°è§¦å‘ runã€‚
        if not self.scheduler._running:
             self._trigger_background_run()
        else:
            # å¦‚æœæ­£åœ¨è¿è¡Œä½†åœ¨æš‚åœ/ç©ºé—²ï¼Œå¯èƒ½éœ€è¦é€»è¾‘å» notify
            # ç®€åŒ–èµ·è§ï¼Œå‡è®¾ running çŠ¶æ€ä¸‹å®ƒåªè·‘åˆå§‹é‚£æ‰¹ã€‚
            # scheduler.run é€»è¾‘æ˜¯: pending_tasks = [t for t in tasks if pending]; await gather()
            # æ‰€ä»¥æ­£åœ¨è¿è¡Œçš„ scheduler ä¸ä¼šåŠ¨æ€æ„ŸçŸ¥çŠ¶æ€å˜å› pending çš„ä»»åŠ¡ã€‚
            # Hack: å¦‚æœæ­£åœ¨è¿è¡Œï¼Œå¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨ä¸€æ¬¡è¿è¡Œå¾ªç¯ï¼Œæˆ–è€… scheduler åº”è¯¥è®¾è®¡ä¸º while loopã€‚
            # ç›®å‰ scheduler.run æ˜¯ç®€å•çš„ä¸€æ³¢æµã€‚
            # æ‰€ä»¥å¦‚æœæ˜¯ Running çŠ¶æ€ï¼ˆæ¯”å¦‚è¿˜æœ‰å…¶ä»–ä»»åŠ¡åœ¨è·‘ï¼‰ï¼Œæˆ‘ä»¬å¾ˆéš¾æ’å…¥ã€‚
            # ä½†ç”¨æˆ·åœºæ™¯é€šå¸¸æ˜¯â€œå…¨éƒ¨è·‘å®Œäº†(failed/completed)â€ï¼Œæ­¤æ—¶ running=Falseã€‚
            # ç›´æ¥è°ƒç”¨ _trigger_background_run å³å¯ã€‚
             pass

    def force_refresh_status(self):
        """å¼ºåˆ¶åˆ·æ–°çŠ¶æ€ç¼“å­˜"""
        self._last_summary = None
        self._last_summary_time = 0

    def _trigger_background_run(self):
        """å¯åŠ¨åå°è¿è¡Œ"""
        threading.Thread(target=self._run_async_tasks, daemon=True).start()

    async def _run_scheduler(self):
        """å¼‚æ­¥æ‰§è¡Œè°ƒåº¦"""
        try:
            await self.scheduler.run()
        except Exception as e:
            logger.error(f"Scheduler failed: {e}")

    def _run_async_tasks(self):
        """åå°è¿è¡Œè°ƒåº¦ä»»åŠ¡"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            # ä½¿ç”¨ create_task åŒ…è£…
            loop.run_until_complete(self._run_scheduler())
        finally:
            loop.close()
            # âš¡ ä¸‹è½½å®Œæˆåè‡ªåŠ¨æ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®èµ„äº§æ¦‚è§ˆæ˜¾ç¤ºæœ€æ–°çŠ¶æ€
            self.force_refresh_status()
            logger.info("ğŸ“Š Download batch completed, status cache refreshed.")

    def get_status(self, audit: bool = False) -> Dict:
        """è·å–ç³»ç»Ÿå…¨é¢çŠ¶æ€ (å¸¦ç¼“å­˜é¿å…é˜»å¡ UI)"""
        import time
        # å¼ºåˆ¶å®¡è®¡æˆ–è¿‡æœŸ
        if audit or self._last_summary is None or (time.time() - self._last_summary_time > 10):
            self._last_summary = self.storage.get_summary(fast=not audit, audit=audit)
            self._last_summary_time = time.time()
            
        return {
            "storage": self._last_summary,
            "download": self.scheduler.get_progress(),
            "slots": self.scheduler._slots
        }

def get_lake_manager() -> LakeManager:
    """è·å–å•ä¾‹å®ä¾‹"""
    return LakeManager()
