import os
import threading
import asyncio
import logging
from typing import Optional, List, Dict
import datetime
from datetime import date
from .storage import LakeStorage
from .downloader import LakeTaskScheduler

logger = logging.getLogger(__name__)

class LakeManager:
    """
    Data Lake V2 ç®¡ç†ä¸­å¿ƒ
    èšåˆå­˜å‚¨ã€ä¸‹è½½ã€è°ƒåº¦ç­‰åŠŸèƒ½ã€‚
    """
    _instance: Optional['LakeManager'] = None
    _lock = threading.Lock()

    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(LakeManager, cls).__new__(cls)
                cls._instance._initialized = False
            return cls._instance

    def __init__(self):
        if self._initialized:
            return
            
        self.storage = LakeStorage()
        self.scheduler = LakeTaskScheduler(self.storage, max_workers=15)
        # åˆå§‹åŒ–é»˜è®¤ç¼“å­˜ï¼Œé˜²æ­¢åŠ è½½æœŸé—´å‡ºç° None å¼•ç”¨
        self._status_cache = {
            "storage": {"total_files": 0, "total_size_mb": 0.0, "pairs": {}},
            "updated_at": "Initializing..."
        }
        self._is_auditing = False
        self._initialized = True
        
        # âš¡ å¯åŠ¨æ—¶è‡ªåŠ¨æ‰§è¡ŒèƒŒæ™¯å®¡è®¡ (å¿«é€Ÿæ¨¡å¼ï¼Œä¸é˜»å¡ UI)
        self.refresh_status(audit=False)

    async def get_top_pairs(self, limit: int = 100, rank_type: str = "market_cap") -> List[str]:
        """è·å–å¸‚åœºæ’åäº¤æ˜“å¯¹ï¼Œæ”¯æŒå¸‚å€¼ (market_cap) å’Œæˆäº¤é¢ (volume)"""
        from .fetcher import BinanceFetcher
        # è¿™é‡Œé»˜è®¤å°è¯•ä½¿ç”¨å®¿ä¸»æœºä»£ç†ä»¥é˜² API è¿æ¥å¤±è´¥
        fetcher = BinanceFetcher(proxy_config="http://host.docker.internal:7890")
        if rank_type == "market_cap":
            return await fetcher.get_market_cap_pairs(limit)
        else:
            return await fetcher.get_top_trading_pairs(limit)

    def start_download(self, pairs: List[str], intervals: List[str], start_date: date, end_date: date):
        """
        çµæ´»ä¸‹è½½å…¥å£ï¼šæ”¯æŒå¤šå¸ç§ã€å¤šå‘¨æœŸã€ä»»æ„æ—¥æœŸã€‚
        """
        def _bg_run():
            self.scheduler.add_tasks(pairs, intervals, start_date, end_date)
            self._run_async_tasks()
            
        threading.Thread(target=_bg_run, daemon=True).start()

    def stop_download(self):
        """å¼ºåˆ¶ç»ˆæ­¢ä¸‹è½½ä»»åŠ¡"""
        self.scheduler.cancel_tasks()

    def pause_download(self):
        """æš‚åœä¸‹è½½"""
        self.scheduler.pause_tasks()

    def resume_download(self):
        """æ¢å¤ä¸‹è½½"""
        self.scheduler.resume_tasks()

    def is_paused(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¤„äºæš‚åœçŠ¶æ€"""
        return not self.scheduler._pause_event.is_set()

    def auto_fill_history(self, pairs: List[str], intervals: List[str], years: int = 3):
        """
        ä¸€é”®è¡¥é½å†å²æ•°æ®é€»è¾‘ã€‚
        """
        def _bg_run():
            # auto_fill_gaps ç°åœ¨æ˜¯åŒæ­¥çš„æˆ–ç®€å•çš„åŒ…è£…
            asyncio.run(self.scheduler.auto_fill_gaps(pairs, intervals, years))
            self._run_async_tasks()
            
        threading.Thread(target=_bg_run, daemon=True).start()

    def repair_all_assets(self):
        """
        ğŸš¨ ä¸€é”®ä¿®å¤æ‰€æœ‰å­˜é‡èµ„äº§
        éå†æ‰€æœ‰ç°æœ‰è®°å½•ï¼Œæ‰¾å‡ºç¼ºå¤±å¤©æ•°å’Œè¡Œæ•°ä¸è¶³çš„å¼‚å¸¸å¤©æ•°ï¼Œå¹¶è§¦å‘ä¸‹è½½ã€‚
        """
        def _bg_repair():
            # 1. è§¦å‘ä¸€æ¬¡åŒæ­¥/æ·±åº¦è¯Šæ–­ (åœ¨åå°çº¿ç¨‹å†…æ‰§è¡Œä»¥é˜²é˜»å¡)
            summary = self.storage.get_summary(fast=False, audit=True)
            pairs_stats = summary.get("pairs", {})

            if not pairs_stats:
                logger.info("No assets found to repair.")
                return

            added_count = 0
            for key, p_stats in pairs_stats.items():
                # key æ ¼å¼ä¸º "binance:BTC-USDT:1m"
                parts = key.split(":")
                if len(parts) < 3: continue
                exch, pair, interval = parts[0], parts[1], parts[2]
                
                # æ‰¾å‡ºç”±äº Gap å¯¼è‡´çš„å®Œå…¨ç¼ºå¤±æ—¥æœŸ
                missing_days = self.storage.get_missing_days(exch, pair, interval)
                
                # æ‰¾å‡ºå®¡è®¡å‘ç°çš„è¡Œæ•°ä¸è¶³çš„å¼‚å¸¸æ—¥æœŸ
                incomplete_days = []
                for day_str in p_stats.get("incomplete_list", []):
                    try:
                        day = date.fromisoformat(day_str)
                        incomplete_days.append(day)
                    except: continue
                
                # æ±‡æ€»éœ€è¦ä¿®å¤çš„ä»»åŠ¡
                all_repair_days = sorted(list(set(missing_days + incomplete_days)))
                
                if all_repair_days:
                    from .downloader import LakeDownloadTask
                    for day in all_repair_days:
                        is_duplicate = any(t.trading_pair == pair and t.interval == interval and t.day == day for t in self.scheduler.tasks)
                        if not is_duplicate:
                            self.scheduler.tasks.append(LakeDownloadTask(trading_pair=pair, interval=interval, day=day))
                            added_count += 1
            
            if added_count > 0:
                logger.info(f"ğŸš¨ Global Repair: Added {added_count} incremental tasks.")
                self._run_async_tasks()
            else:
                logger.info("âœ… Global Repair: All assets are healthy. No tasks added.")

        threading.Thread(target=_bg_repair, daemon=True).start()

    def _trigger_background_run(self):
        """å¯åŠ¨åå°è¿è¡Œ"""
        if not self.scheduler._running:
            threading.Thread(target=self._run_async_tasks, daemon=True).start()

    async def _run_scheduler(self):
        """å¼‚æ­¥æ‰§è¡Œè°ƒåº¦"""
        try:
            await self.scheduler.run()
        except Exception as e:
            logger.error(f"Scheduler failed: {e}")

    def _run_async_tasks(self):
        """åå°è¿è¡Œè°ƒåº¦ä»»åŠ¡"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(self._run_scheduler())
        finally:
            loop.close()
            # âš¡ ä¸‹è½½å®Œæˆåè‡ªåŠ¨è§¦å‘å¢é‡åˆ·æ–°
            self.refresh_status(audit=False)
            logger.info("ğŸ“Š Download batch completed, incremental status refresh triggered.")

    def retry_failed_tasks(self):
        """é‡è¯•æ‰€æœ‰å¤±è´¥çš„ä»»åŠ¡"""
        failed_tasks = [t for t in self.scheduler.tasks if t.status == "failed"]
        if not failed_tasks:
            logger.info("No failed tasks to retry.")
            return
            
        logger.info(f"Retrying {len(failed_tasks)} failed tasks...")
        for t in failed_tasks:
            t.status = "pending"
            t.error = None
            t.rows_downloaded = 0
            
        # ç¡®ä¿è°ƒåº¦å™¨å¯ä»¥è¿è¡Œ (å¦‚æœæ˜¯åœæ­¢çŠ¶æ€)
        self.scheduler._stop_signal = False
        self.scheduler._pause_event.set()
        
        # å¯åŠ¨åå°çº¿ç¨‹ (å¦‚æœå·²æœ‰çº¿ç¨‹åœ¨è·‘ _run_scheduler ä¼šè‡ªåŠ¨æ¡èµ· pending ä»»åŠ¡å—ï¼Ÿ
        # å–å†³äº scheduler.run å®ç°ã€‚scheduler.run æ˜¯ä¸€æ¬¡æ€§çš„ gather(pending)ï¼Œè·‘å®Œå°±é€€å‡ºäº†ã€‚
        # æ‰€ä»¥å¿…é¡»é‡æ–°è§¦å‘ runã€‚
        if not self.scheduler._running:
             self._trigger_background_run()
        else:
            # å¦‚æœæ­£åœ¨è¿è¡Œä½†åœ¨æš‚åœ/ç©ºé—²ï¼Œå¯èƒ½éœ€è¦é€»è¾‘å» notify
            # ç®€åŒ–èµ·è§ï¼Œå‡è®¾ running çŠ¶æ€ä¸‹å®ƒåªè·‘åˆå§‹é‚£æ‰¹ã€‚
            # scheduler.run é€»è¾‘æ˜¯: pending_tasks = [t for t in tasks if pending]; await gather()
            # æ‰€ä»¥æ­£åœ¨è¿è¡Œçš„ scheduler ä¸ä¼šåŠ¨æ€æ„ŸçŸ¥çŠ¶æ€å˜å› pending çš„ä»»åŠ¡ã€‚
            # Hack: å¦‚æœæ­£åœ¨è¿è¡Œï¼Œå¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨ä¸€æ¬¡è¿è¡Œå¾ªç¯ï¼Œæˆ–è€… scheduler åº”è¯¥è®¾è®¡ä¸º while loopã€‚
            # ç›®å‰ scheduler.run æ˜¯ç®€å•çš„ä¸€æ³¢æµã€‚
            # æ‰€ä»¥å¦‚æœæ˜¯ Running çŠ¶æ€ï¼ˆæ¯”å¦‚è¿˜æœ‰å…¶ä»–ä»»åŠ¡åœ¨è·‘ï¼‰ï¼Œæˆ‘ä»¬å¾ˆéš¾æ’å…¥ã€‚
            # ä½†ç”¨æˆ·åœºæ™¯é€šå¸¸æ˜¯â€œå…¨éƒ¨è·‘å®Œäº†(failed/completed)â€ï¼Œæ­¤æ—¶ running=Falseã€‚
            # ç›´æ¥è°ƒç”¨ _trigger_background_run å³å¯ã€‚
             pass

    def refresh_status(self, audit: bool = False):
        """
        è§¦å‘çŠ¶æ€æ›´æ–°ã€‚å¦‚æœæ˜¯ audit=Trueï¼Œä¼šåœ¨åå°æ‰§è¡Œæ·±åº¦æ‰«æã€‚
        """
        if self._is_auditing:
            return

        def _bg_scan():
            self._is_auditing = True
            try:
                # æ‰§è¡Œæ‰«æ
                summary = self.storage.get_summary(fast=not audit, audit=audit)
                # æ›´æ–°ç¼“å­˜
                self._status_cache = {
                    "storage": summary,
                    "updated_at": datetime.datetime.now().strftime("%H:%M:%S")
                }
                logger.info(f"ğŸ“Š Lake status updated (audit={audit})")
            except Exception as e:
                logger.error(f"âŒ Status scan failed: {e}")
            finally:
                self._is_auditing = False

        threading.Thread(target=_bg_scan, daemon=True).start()

    def get_status(self, audit: bool = False) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€ (ä¼˜å…ˆä½¿ç”¨ç¼“å­˜)"""
        # å¦‚æœè°ƒç”¨è€…æ˜ç¡®è¦æ±‚ audit ä¸”å½“å‰æœªåœ¨å®¡è®¡ä¸­ï¼Œåˆ™è§¦å‘ä¸€æ¬¡èƒŒæ™¯å®¡è®¡
        if audit and not self._is_auditing:
            self.refresh_status(audit=True)

        return {
            "storage": self._status_cache["storage"] if self._status_cache else {"total_files": 0, "total_size_mb": 0.0, "pairs": {}},
            "download": self.scheduler.get_progress(),
            "slots": self.scheduler._slots,
            "is_auditing": self._is_auditing,
            "last_updated": self._status_cache["updated_at"] if self._status_cache else "Scanning..."
        }

def get_lake_manager() -> LakeManager:
    """è·å–å•ä¾‹å®ä¾‹"""
    return LakeManager()
