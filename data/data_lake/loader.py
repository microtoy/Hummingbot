import pandas as pd
from pathlib import Path
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor
from typing import List, Optional
import hummingbot

class LakeLoader:
    """
    Data Lake V2 é«˜æ€§èƒ½åŠ è½½å™¨
    ç›´æ¥ä»åˆ†ç‰‡å­˜å‚¨ä¸­è¯»å–æ•°æ®ï¼Œæ”¯æŒå¹¶è¡Œ IO ä»¥æœ€å¤§åŒ–ååé‡ã€‚
    """
    def __init__(self, base_path: Optional[str] = None):
        if base_path:
            self.base_path = Path(base_path)
        else:
            # âš¡ Turbo æ¨¡å¼è¡¥ä¸: Turbo Worker ä¼šå°† data_path è®¾ä¸º /tmp/hbot_data
            # ä½† Lake æ•°æ®é€šå¸¸å¾ˆå¤§ï¼Œä¸ä¼šè¢«é•œåƒåˆ° tmpï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ£€æµ‹å¹¶å›é€€ã€‚
            hbot_path = Path(hummingbot.data_path())
            lake_path = hbot_path / "lake"
            
            if not lake_path.exists():
                # å°è¯•ä»åº“å®‰è£…è·¯å¾„å¯»æ‰¾åŸå§‹æ•°æ®
                try:
                    original_base = Path(hummingbot.prefix_path()) / "data" / "lake"
                    if original_base.exists():
                        lake_path = original_base
                except:
                    pass
            
            self.base_path = lake_path

    def _get_path(self, exchange: str, pair: str, interval: str, day: date) -> Path:
        """æ ¹æ®å­˜å‚¨è§„åˆ™æ„å»ºè·¯å¾„"""
        path = self.base_path / exchange / pair / interval / str(day.year) / f"{day.month:02d}" / f"{day.isoformat()}.csv"
        return path

    def get_data(self, exchange: str, pair: str, interval: str, start_ts: int, end_ts: int, workers: int = 8) -> pd.DataFrame:
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šå¹¶è¡Œè¯»å–æŒ‡å®šèŒƒå›´çš„æ•°æ®
        """
        start_date = datetime.fromtimestamp(start_ts).date()
        end_date = datetime.fromtimestamp(end_ts).date()
        
        # 1. ç”Ÿæˆæ—¥æœŸåºåˆ—
        target_days = []
        curr = start_date
        while curr <= end_date:
            target_days.append(curr)
            curr += timedelta(days=1)
            
        # 2. ç­›é€‰å­˜åœ¨çš„æ–‡ä»¶
        tasks = []
        is_synthesized = False
        
        for i, day in enumerate(target_days):
            path = self._get_path(exchange, pair, interval, day)
            exists = path.exists()
            if i < 3:
                print(f"DEBUG: Checking {path} -> {exists}")
            if exists:
                tasks.append(path)
                
        # ğŸŒŸ SPECIAL: å¦‚æœæ²¡æ‰¾åˆ° 4h æ•°æ®ï¼Œä¸”æœ‰ 1h æ•°æ®ï¼Œåˆ™è‡ªåŠ¨è¿›è¡Œé™é‡‡æ ·åˆæˆ
        if not tasks and interval == "4h":
            print(f"âš ï¸ [LOADER SYNTHESIS] No 4h shards found for {exchange}:{pair}. Trying to synthesize from 1h.")
            for day in target_days:
                path = self._get_path(exchange, pair, "1h", day)
                if path.exists():
                    tasks.append(path)
            if tasks:
                is_synthesized = True

        if not tasks:
            print(f"âŒ [LOADER DEBUG] No tasks generated for {exchange}:{pair}:{interval}. Total days checked: {len(target_days)}. First path: {self._get_path(exchange, pair, interval, target_days[0]) if target_days else 'N/A'}")
            return pd.DataFrame()
            
        # 3. å¹¶è¡Œè¯»å–
        def read_one(path):
            try:
                df = pd.read_csv(path)
                # ğŸ”¥ FIX: Normalize timestamp immediately (early data uses milliseconds)
                if 'timestamp' in df.columns and df['timestamp'].max() > 1e11:
                    df['timestamp'] = df['timestamp'] / 1000.0
                return df
            except Exception as e:
                print(f"âš ï¸ [LOADER ERROR] Failed to read {path}: {e}")
                return None

        with ThreadPoolExecutor(max_workers=workers) as pool:
            results = list(pool.map(read_one, tasks))
            
        # 4. åˆå¹¶å¹¶è¿‡æ»¤ç²¾ç¡®èŒƒå›´
        dfs = [df for df in results if df is not None and not df.empty]
        if not dfs:
            return pd.DataFrame()
            
        full_df = pd.concat(dfs, ignore_index=True)
        
        # 5. æ’åºä¸å»é‡
        full_df = full_df.sort_values("timestamp").drop_duplicates(subset=["timestamp"])

        # 6. è®¡ç®—é™é‡‡æ · (å¦‚æœéœ€è¦)
        if is_synthesized and interval == "4h":
            full_df = self._resample_1h_to_4h(full_df)

        # 7. è¿‡æ»¤ç²¾ç¡®èŒƒå›´
        mask = (full_df["timestamp"] >= start_ts) & (full_df["timestamp"] <= end_ts)
        full_df = full_df[mask]
        
        return full_df.reset_index(drop=True)

    def _resample_1h_to_4h(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä» 1h Kçº¿é™é‡‡æ ·åˆæˆ 4h Kçº¿"""
        if df.empty:
            return df
        
        # è½¬æ¢ä¸º datetime ä»¥ä¾¿ resample
        df = df.copy()
        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')
        df = df.set_index('datetime')
        
        # æ ‡å‡† OHLCV èšåˆ
        resampled = df.resample('4h', label='left', closed='left').agg({
            'timestamp': 'first',
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna()
        
        # ç¡®ä¿ timestamp ä¾ç„¶æ˜¯æ•´æ•°
        resampled['timestamp'] = resampled['timestamp'].astype(int)
        
        return resampled.reset_index(drop=True)
