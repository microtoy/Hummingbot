import os
import pandas as pd
from datetime import date, timedelta, datetime
from typing import List, Optional
from pathlib import Path
import logging

from .storage import LakeStorage

logger = logging.getLogger(__name__)

class DataMerger:
    """
    è´Ÿè´£å°†åˆ†ç‰‡æ•°æ® (Partitions) åˆå¹¶ä¸ºå•ä¸€ CSV æ–‡ä»¶ï¼Œä»¥å…¼å®¹ Hummingbotã€‚
    """
    def __init__(self, storage: LakeStorage):
        self.storage = storage

    def merge_to_legacy(self, exchange: str, trading_pair: str, interval: str, 
                        output_path: str, start_date: date, end_date: date):
        """
        å°†é€‰å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®åˆå¹¶å¹¶å¯¼å‡ºä¸º Hummingbot æ ¼å¼ã€‚
        """
        all_dfs = []
        current = start_date
        while current <= end_date:
            df = self.storage.load_day_data(exchange, trading_pair, interval, current)
            if df is not None:
                # ğŸ”¥ FIX: Normalize timestamp IMMEDIATELY after loading each shard
                if 'timestamp' in df.columns:
                    # Early data (2017-2019) uses milliseconds, later data uses seconds
                    if df['timestamp'].max() > 1e11:  # Milliseconds detected
                        df['timestamp'] = df['timestamp'] / 1000.0
                all_dfs.append(df)
            current += timedelta(days=1)
            
        if not all_dfs:
            logger.warning(f"No data found in lake for {trading_pair} between {start_date} and {end_date}")
            return False
            
        # 1. åˆå¹¶
        merged_df = pd.concat(all_dfs, ignore_index=True)
        
        # 2. å½’ä¸€åŒ–æ ¡éªŒ (å·²åœ¨ä¸Šé¢å¤„ç†ï¼Œè¿™é‡Œä»…åšæœ€ç»ˆéªŒè¯)
        if 'timestamp' in merged_df.columns:
            # Double-check: should all be in seconds now
            if merged_df['timestamp'].max() > 1e11:
                merged_df['timestamp'] = merged_df['timestamp'] / 1000.0
            
            # å…¼å®¹æ€§æ˜ å°„ï¼šå¤„ç†æ—§æ ¼å¼åˆ†ç‰‡ä¸­çš„é•¿åˆ—å
            compat_map = {
                "number_of_trades": "n_trades",
                "taker_buy_base_asset_volume": "taker_buy_base_volume",
                "taker_buy_quote_asset_volume": "taker_buy_quote_volume"
            }
            for old_col, new_col in compat_map.items():
                if old_col in merged_df.columns and new_col not in merged_df.columns:
                    merged_df[new_col] = merged_df[old_col]
            
            # ç»Ÿä¸€åˆ—åä¸º V1 è§„èŒƒ
            v1_columns = [
                "timestamp", "open", "high", "low", "close", "volume",
                "quote_asset_volume", "n_trades", "taker_buy_base_volume", "taker_buy_quote_volume"
            ]
            # ä»…ä¿ç•™å­˜åœ¨çš„ V1 åˆ—å¹¶è¡¥é½
            existing_v1 = [c for c in v1_columns if c in merged_df.columns]
            merged_df = merged_df[existing_v1]
            for col in v1_columns:
                if col not in merged_df.columns:
                    merged_df[col] = 0.0
            merged_df = merged_df[v1_columns] # å¼ºåˆ¶é¡ºåº

        # 3. å»é‡ä¸ç‰©ç†æ’åº
        merged_df.drop_duplicates(subset=['timestamp'], keep='last', inplace=True)
        merged_df.sort_values(by='timestamp', inplace=True)
        
        # åŸå­å†™å…¥
        out_p = Path(output_path)
        out_p.parent.mkdir(parents=True, exist_ok=True)
        
        tmp_p = out_p.with_suffix(".tmp_merge")
        merged_df.to_csv(tmp_p, index=False)
        tmp_p.replace(out_p)
        
        logger.info(f"âœ… Merged {len(merged_df)} rows for {trading_pair} into {output_path}")
        return True

    def auto_merge_full_history(self, exchange: str, trading_pair: str, interval: str, output_path: str):
        """
        è‡ªåŠ¨å‘ç°è¯¥å¸ç§åœ¨ Lake ä¸­çš„æ‰€æœ‰æ•°æ®å¹¶åˆå¹¶ã€‚
        """
        existing_days = self.storage.list_existing_days(exchange, trading_pair, interval)
        if not existing_days:
            return False
            
        return self.merge_to_legacy(
            exchange, trading_pair, interval, output_path,
            start_date=existing_days[0],
            end_date=existing_days[-1]
        )
