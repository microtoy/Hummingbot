
import argparse
import requests
import json
import random
import time
import os
import sys
import threading
import concurrent.futures
from datetime import datetime, timedelta
import pandas as pd
from requests.auth import HTTPBasicAuth

# --- CONFIGURATION ---
API_URL = "http://hummingbot-api:8000/backtesting/batch-run"
AUTH = HTTPBasicAuth("admin", "admin")
OUTPUT_DIR = "/home/dashboard/custom_strategies/optimization_reports"
TOP_10_TOKENS = [
    "BTC-USDT", "ETH-USDT", "BNB-USDT", "XRP-USDT", "SOL-USDT",
    "ADA-USDT", "AVAX-USDT", "DOGE-USDT", "LINK-USDT", "TRX-USDT"
]

class StrategyOptimizer:
    def __init__(self, mode, days=730, iterations=30, batch_size=None, workers=None):
        self.mode = mode
        self.days = days
        self.iterations = iterations
        
        # Auto-detect CPU power
        cpu_count = os.cpu_count() or 4
        
        # ============ PERFORMANCE TUNING GUIDE ============
        # Backend API uses ProcessPoolExecutor(max_workers=10)
        # Each API request spawns 10 parallel processes, each processing batch_size/10 sims
        #
        # Key Formula: Total Concurrent Processes = workers * 10
        # - workers=1 -> 10 processes (optimal CPU utilization for 10-core)
        # - workers=2 -> 20 processes (may cause contention)
        # - workers=4 -> 40 processes (severe slowdown due to context switching)
        #
        # RECOMMENDED SETTINGS for 10-core / 64GB RAM:
        # - workers=1, batch_size=50  -> 10 parallel sims, sequential batches (~60s/batch)
        # - workers=2, batch_size=30  -> 20 parallel sims, slight overlap (~45s/batch)
        #
        # Your test results:
        # - workers=1, batch_size=50: ~57s/batch (OPTIMAL - no contention)
        # - workers=4, batch_size=50: ~205s/batch (4x slower due to 40 processes fighting for 10 cores)
        # ===================================================
        
        # Optimal Configuration: Let backend fully utilize its 10-core pool
        self.batch_size = batch_size if batch_size else 50
        self.workers = workers if workers else 2  # 2 workers = 20 parallel, with pipeline overlap
        
        self.start_ts, self.end_ts = self._get_time_range(days)
        self.report_id = datetime.now().strftime("%Y%m%d_%H%M")
        
        # Ensure output dir exists
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        self.csv_filename = f"{OUTPUT_DIR}/opt_{self.mode}_{self.report_id}.csv"
        self.results_cache = []
        
        # Heartbeat Status Tracking
        self.stats = {
            "batches_sent": 0,
            "batches_received": 0,
            "batches_success": 0,
            "batches_error": 0,
            "sims_completed": 0,
            "sims_error": 0,
            "best_pnl": -999.0,
            "best_pair": "N/A",
            "last_activity": time.time(),
        }
        self.stats_lock = threading.Lock()
        self._heartbeat_running = False
        
    def _get_time_range(self, days, offset_days=365):
        """
        Get start/end timestamps for backtesting.
        
        Default: Use 730 days of data, offset by 365 days
        This means: 2023-01-01 ~ 2024-12-31 (leaving 2025 for final validation)
        
        Timeline:
        - 2018-2020: Walk-Forward validation (historical)
        - 2021-2022: OOS validation (completely unseen)
        - 2023-2024: Parameter optimization ‚óÑ‚îÄ‚îÄ DEFAULT WINDOW
        - 2025-2026: Reserved for final validation
        """
        end_dt = datetime.now() - timedelta(days=offset_days)  # Skip most recent 'offset_days'
        start_dt = end_dt - timedelta(days=days)
        return int(start_dt.timestamp()), int(end_dt.timestamp())

    def generate_discovery_config(self, pair):
        """Variance: High"""
        interval = random.choice(["1h", "4h"])
        fast = random.choice(list(range(5, 60, 5)))
        slow = random.choice([s for s in range(20, 200, 10) if s > fast + 10])
        return {
            "connector_name": "binance",
            "controller_name": "ma_cross_strategy",
            "controller_type": "custom",
            "trading_pair": pair,
            "fast_ma": fast,
            "slow_ma": slow,
            "indicator_interval": interval,
            "stop_loss": random.choice([0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.10]),
            "take_profit": random.choice([0.02, 0.05, 0.08, 0.10, 0.15, 0.20]),
            "time_limit": 21600,
            "total_amount_quote": 100,
            "use_compounding": True
        }

    def generate_refinement_config(self, pair):
        """Variance: Low (Local Search)"""
        # Baseline Center
        base_fast, base_slow = 5, 10
        base_sl, base_tp = 0.02, 0.04
        
        fast = base_fast + random.choice([-1, 0, 1, 2, 3])
        slow = base_slow + random.choice([0, 2, 4, 6])
        if slow <= fast: slow = fast + 5
        
        return {
            "connector_name": "binance",
            "controller_name": "ma_cross_strategy",
            "controller_type": "custom",
            "trading_pair": pair,
            "fast_ma": fast,
            "slow_ma": slow,
            "indicator_interval": "1h",
            "stop_loss": base_sl + random.choice([-0.005, 0, 0.005, 0.01]),
            "take_profit": base_tp + random.choice([-0.01, 0, 0.01, 0.02, 0.03]),
            "time_limit": random.choice([21600, 43200, 86400]),
            "total_amount_quote": 100,
            "use_compounding": True
        }

    def process_batch(self, batch_data):
        """Worker function to process a single batch via API"""
        batch_id, batch_configs = batch_data
        batch_num = batch_id // self.batch_size + 1
        start_time = time.time()
        
        # Update stats: batch sent
        with self.stats_lock:
            self.stats["batches_sent"] += 1
            self.stats["last_activity"] = time.time()
        
        print(f"\n   üì§ Batch {batch_num} sending ({len(batch_configs)} sims)...", flush=True)
        
        try:
            response = requests.post(API_URL, json=batch_configs, auth=AUTH, timeout=600)  # 10 min timeout
            elapsed = time.time() - start_time
            
            if response.status_code != 200:
                with self.stats_lock:
                    self.stats["batches_error"] += 1
                    self.stats["last_activity"] = time.time()
                print(f"\n   ‚ùå Batch {batch_num}: API Error {response.status_code} ({elapsed:.1f}s)", flush=True)
                return []
            
            valid_results = response.json().get("results", [])
            error_count = sum(1 for r in valid_results if "error" in r)
            success_count = len([r for r in valid_results if "error" not in r])
            
            # Update stats: batch received
            with self.stats_lock:
                self.stats["batches_received"] += 1
                self.stats["batches_success"] += 1
                self.stats["sims_completed"] += success_count
                self.stats["sims_error"] += error_count
                self.stats["last_activity"] = time.time()
                
                # Track best result
                for r in valid_results:
                    if "error" not in r:
                        pnl = float(r.get("net_pnl", 0))
                        if pnl > self.stats["best_pnl"]:
                            self.stats["best_pnl"] = pnl
                            self.stats["best_pair"] = r.get("trading_pair", "N/A")
            
            print(f"\n   üì• Batch {batch_num} received: {success_count} ok, {error_count} errors ({elapsed:.1f}s)", flush=True)
            
            # Debug: Check for errors in the results
            for r in valid_results:
                if "error" in r:
                    print(f"\n      ‚ö†Ô∏è {r.get('trading_pair', 'Unknown')}: {str(r['error'])[:60]}...", flush=True)

            return [r for r in valid_results if "error" not in r]
        except requests.exceptions.Timeout:
            elapsed = time.time() - start_time
            with self.stats_lock:
                self.stats["batches_error"] += 1
            print(f"\n   ‚è∞ Batch {batch_num}: Timeout after {elapsed:.1f}s", flush=True)
            return []
        except Exception as e:
            elapsed = time.time() - start_time
            with self.stats_lock:
                self.stats["batches_error"] += 1
            print(f"\n   ‚ùå Batch {batch_num}: Exception {str(e)[:50]}... ({elapsed:.1f}s)", flush=True)
            return []

    def _heartbeat_monitor(self, total_batches, run_start_time):
        """Background thread that prints status every 30 seconds"""
        heartbeat_interval = 30  # seconds
        last_sent = 0
        last_received = 0
        
        while self._heartbeat_running:
            time.sleep(heartbeat_interval)
            if not self._heartbeat_running:
                break
                
            with self.stats_lock:
                stats = self.stats.copy()
            
            elapsed = time.time() - run_start_time
            elapsed_min = int(elapsed // 60)
            elapsed_sec = int(elapsed % 60)
            
            # Calculate rates
            sent_delta = stats["batches_sent"] - last_sent
            recv_delta = stats["batches_received"] - last_received
            last_sent = stats["batches_sent"]
            last_received = stats["batches_received"]
            
            # Time since last activity
            idle_time = time.time() - stats["last_activity"]
            
            # ETA calculation
            if stats["batches_received"] > 0:
                avg_time = elapsed / stats["batches_received"]
                remaining = total_batches - stats["batches_received"]
                eta_sec = remaining * avg_time
                eta_min = int(eta_sec // 60)
            else:
                eta_min = -1
            
            # Print heartbeat status
            print(f"\n{'='*60}", flush=True)
            print(f"üíì HEARTBEAT [{datetime.now().strftime('%H:%M:%S')}] - Elapsed: {elapsed_min}m {elapsed_sec}s", flush=True)
            print(f"‚îú‚îÄ üì§ Sent:     {stats['batches_sent']}/{total_batches} batches (+{sent_delta} in last 30s)", flush=True)
            print(f"‚îú‚îÄ üì• Received: {stats['batches_received']}/{total_batches} batches (+{recv_delta} in last 30s)", flush=True)
            print(f"‚îú‚îÄ ‚úÖ Success:  {stats['sims_completed']} sims | ‚ùå Errors: {stats['sims_error']} sims", flush=True)
            print(f"‚îú‚îÄ üèÜ Best:     {stats['best_pair']} (PnL: {stats['best_pnl']:.2f}%)", flush=True)
            print(f"‚îú‚îÄ ‚è±Ô∏è  Idle:     {idle_time:.1f}s since last activity", flush=True)
            if eta_min >= 0:
                print(f"‚îî‚îÄ ‚è≥ ETA:      ~{eta_min}m remaining", flush=True)
            else:
                print(f"‚îî‚îÄ ‚è≥ ETA:      calculating...", flush=True)
            print(f"{'='*60}", flush=True)
            
            # Warn if idle too long
            if idle_time > 120:
                print(f"\n‚ö†Ô∏è  WARNING: No activity for {idle_time:.0f}s - backend may be stuck!", flush=True)

    def run(self, target_tokens=TOP_10_TOKENS):
        print(f"\nüöÄ STRATEGY OPTIMIZER v3.0 (Parallel Edition)")
        print(f"==========================================")
        print(f"üîπ Mode:       {self.mode.upper()}")
        print(f"üîπ Concurrency:{self.workers} Workers")
        print(f"üîπ Parallel:   {self.batch_size} Simultaneous Sims per Req")
        print(f"üîπ Report ID:  {self.report_id}")
        print(f"==========================================\n")
        
        # CSV Header
        with open(self.csv_filename, 'w') as f:
            f.write("Pair,Interval,Fast,Slow,SL,TP,TL,Net PnL %,Max DD %,Win Rate %,Sharpe\n")
            
        total_configs = []
        for pair in target_tokens:
            for _ in range(self.iterations):
                if self.mode == "refine": cfg = self.generate_refinement_config(pair)
                else: cfg = self.generate_discovery_config(pair)
                
                total_configs.append({
                    "config": cfg,
                    "start_time": self.start_ts,
                    "end_time": self.end_ts,
                    "backtesting_resolution": "1m", # High precision
                    "trade_cost": 0.0006
                })
        
        random.shuffle(total_configs)
        
        # Prepare Batches
        batches = []
        for i in range(0, len(total_configs), self.batch_size):
            batches.append((i, total_configs[i:i+self.batch_size]))
            
        total_sims = len(total_configs)
        print(f"‚ö° Dispatching {len(batches)} batches across {self.workers} threads...", flush=True)
        print(f"üìä Total simulations: {total_sims} | Tokens: {len(target_tokens)} | Iters/Token: {self.iterations}", flush=True)
        print(f"‚è±Ô∏è  Start time: {datetime.now().strftime('%H:%M:%S')}", flush=True)
        print(f"{'='*60}", flush=True)

        # --- PARALLEL EXECUTION LOOP ---
        run_start_time = time.time()
        completed_batches = 0
        
        # Start heartbeat monitor thread
        self._heartbeat_running = True
        heartbeat_thread = threading.Thread(
            target=self._heartbeat_monitor,
            args=(len(batches), run_start_time),
            daemon=True
        )
        heartbeat_thread.start()
        print(f"üíì Heartbeat monitor started (reports every 30s)", flush=True)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.workers) as executor:
            # map returns an iterator in order, but we can't show progress easily with map
            # use as_completed
            futures = {executor.submit(self.process_batch, b): b for b in batches}
            
            for future in concurrent.futures.as_completed(futures):
                results = future.result()
                completed_batches += 1
                
                # Save & Cache Results
                rows = []
                for res in results:
                    c = res.get("config", {})
                    pnl_pct = res['net_pnl'] * 100
                    dd_pct = res['max_drawdown_pct'] * 100
                    
                    self.results_cache.append({
                        "Pair": res['trading_pair'],
                        "Config": f"Fast {c.get('fast_ma')}/Slow {c.get('slow_ma')}/{c.get('indicator_interval')} (SL {c.get('stop_loss')}/TP {c.get('take_profit')})",
                        "PnL": pnl_pct,
                        "Drawdown": dd_pct,
                        "Sharpe": res['sharpe_ratio'],
                    })
                    
                    row = f"{res['trading_pair']},{c.get('indicator_interval')},{c.get('fast_ma')},{c.get('slow_ma')},{c.get('stop_loss')},{c.get('take_profit')},{c.get('time_limit')},{pnl_pct:.2f},{dd_pct:.2f},{res['accuracy']*100:.2f},{res['sharpe_ratio']:.2f}"
                    rows.append(row)
                
                if rows:
                    with open(self.csv_filename, 'a') as f:
                        for r in rows: f.write(r + "\n")
                
                # Calculate timing
                elapsed_total = time.time() - run_start_time
                avg_batch_time = elapsed_total / completed_batches if completed_batches > 0 else 0
                remaining_batches = len(batches) - completed_batches
                eta_seconds = remaining_batches * avg_batch_time
                eta_min = int(eta_seconds // 60)
                eta_sec = int(eta_seconds % 60)
                
                # [NEW] Real-time Batch Feedback
                best_roi = -999.0
                best_pair = "N/A"
                if results:
                    for res in results:
                        pnl = float(res.get("net_pnl", 0))
                        if pnl > best_roi:
                            best_roi = pnl
                            best_pair = res.get("trading_pair", "N/A")
                            
                    print(f"\n   ‚úÖ Batch {completed_batches}/{len(batches)} Done: {len(results)} sims. Best: {best_pair} (PnL {best_roi:.2f})")
                
                pct = completed_batches / len(batches) * 100
                bar_len = 30
                filled = int(bar_len * pct / 100)
                bar = '‚ñà' * filled + '-' * (bar_len - filled)
                
                # Show progress with ETA
                print(f"\nüöÄ Progress: [{bar}] {pct:.1f}% ({completed_batches}/{len(batches)}) | ETA: {eta_min}m {eta_sec}s", flush=True)
                print(f"‚è±Ô∏è  Elapsed: {int(elapsed_total//60)}m {int(elapsed_total%60)}s | Avg: {avg_batch_time:.1f}s/batch", flush=True)

                # Periodic Memory Cleanup (Every 1 batches)
                if completed_batches % 1 == 0:
                    self.clean_server_memory()
        
        # Stop heartbeat monitor
        self._heartbeat_running = False
        print(f"\nüíì Heartbeat monitor stopped", flush=True)
        
        print(f"\n‚úÖ Optimization Complete.")
        self.generate_report(self.results_cache)

    def clean_server_memory(self):
        """Trigger backend garbage collection"""
        try:
            requests.post(API_URL.replace("/batch-run", "/gc"), auth=AUTH, timeout=2)
        except: pass

    def generate_report(self, results):
        if not results:
            print("‚ùå No results to report.")
            return

        df = pd.DataFrame(results)
        
        # Find "Holy Grails" (>15% PnL, <20% DD)
        grails = df[(df["PnL"] > 15) & (df["Drawdown"].abs() < 20)].sort_values("PnL", ascending=False)
        
        # Find "Stable Gems" (PnL > 0, DD < 10%, Sharpe > 1.5)
        gems = df[(df["PnL"] > 0) & (df["Drawdown"].abs() < 10) & (df["Sharpe"] > 1.5)].sort_values("Sharpe", ascending=False)

        md_filename = f"{OUTPUT_DIR}/report_{self.mode}_{self.report_id}.md"
        
        with open(md_filename, 'w') as f:
            f.write(f"# üìä Strategy Optimization Report: {self.mode.title()}\n\n")
            f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
            f.write(f"**Analysis Window**: Last {self.days} days\n")
            f.write(f"**Resolution**: 1m (High Precision)\n\n")
            
            f.write("## üèÜ Top Performers (Holy Grails)\n")
            f.write("> **Criteria**: PnL > 15% , Max Drawdown < 20%\n\n")
            if not grails.empty:
                f.write(grails.head(10)[["Pair", "Config", "PnL", "Drawdown", "Sharpe"]].to_markdown(index=False, floatfmt=".2f"))
            else:
                f.write("*No strategies met the strict 'Holy Grail' criteria in this run.*")
            
            f.write("\n\n## üíé Low Risk Gems (High Stability)\n")
            f.write("> **Criteria**: PnL > 0%, Max Drawdown < 10%, Sharpe > 1.5\n\n")
            if not gems.empty:
                f.write(gems.head(10)[["Pair", "Config", "PnL", "Drawdown", "Sharpe"]].to_markdown(index=False, floatfmt=".2f"))
            else:
                f.write("*No ultra-stable strategies found.*")
                
            f.write("\n\n## üìã Parameter Instructions\n")
            f.write("Apply these best params in **Dashboard -> Smart Strategy**.\n")
            
        print(f"üìù Report generated: {md_filename}")
        if not grails.empty:
            print("\nüèÜ Top Result Preview:")
            print(grails.head(3).to_string(index=False))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Hummingbot V2 Strategy Optimizer")
    parser.add_argument("--mode", type=str, choices=["discovery", "refine"], default="discovery")
    parser.add_argument("--days", type=int, default=90)
    parser.add_argument("--iter", type=int, default=10)
    parser.add_argument("--tokens", type=str, default="ALL")
    parser.add_argument("--batch_size", type=int, default=None, help="Backtest scenarios per API call")
    parser.add_argument("--workers", type=int, default=None, help="Parallel API connections")
    
    args = parser.parse_args()
    tokens = TOP_10_TOKENS if args.tokens == "ALL" else args.tokens.split(",")
    
    optimizer = StrategyOptimizer(mode=args.mode, days=args.days, iterations=args.iter, batch_size=args.batch_size, workers=args.workers)
    optimizer.run(target_tokens=tokens)
